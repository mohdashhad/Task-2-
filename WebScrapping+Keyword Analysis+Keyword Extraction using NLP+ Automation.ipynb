{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bdb8e-019a-42d4-9ba6-8b834258e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.tripadvisor.in/Tourism-g295424-Dubai_Emirate_of_Dubai-Vacations.html\",\n",
    "    \"https://thesophisticatedlife.com/blog/dubai-travel-guide-first-time-visitors/\",\n",
    "    \"https://www.expedia.co.in/Dubai.dx6053839\"\n",
    "]\n",
    "\n",
    "# Function to extract visible text\n",
    "def get_visible_text(soup):\n",
    "    for script in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        script.decompose()\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "# Function to scrape each website\n",
    "def scrape_website(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "    data = {\n",
    "        \"title\": soup.title.string.strip() if soup.title else \"\",\n",
    "        \"meta_desc\": \"\",\n",
    "        \"h1\": \" | \".join([h.get_text(strip=True) for h in soup.find_all(\"h1\")]),\n",
    "        \"h2\": \" | \".join([h.get_text(strip=True) for h in soup.find_all(\"h2\")]),\n",
    "        \"h3\": \" | \".join([h.get_text(strip=True) for h in soup.find_all(\"h3\")]),\n",
    "        \"body\": get_visible_text(soup)\n",
    "    }\n",
    "\n",
    "    # Meta description\n",
    "    meta_tag = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    if meta_tag and meta_tag.get(\"content\"):\n",
    "        data[\"meta_desc\"] = meta_tag[\"content\"].strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "# Collect all content\n",
    "combined_text = \"\"\n",
    "for url in urls:\n",
    "    scraped = scrape_website(url)\n",
    "    combined_text += \" \".join(scraped.values()) + \" \"\n",
    "\n",
    "# Preprocess and tokenize\n",
    "words = nltk.word_tokenize(combined_text.lower())\n",
    "filtered = [\n",
    "    w for w in words if w.isalpha() and w not in stop_words and len(w) > 2\n",
    "]\n",
    "\n",
    "# Generate bigrams and trigrams\n",
    "def generate_ngrams(tokens, n):\n",
    "    return list(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "bigrams = generate_ngrams(filtered, 2)\n",
    "trigrams = generate_ngrams(filtered, 3)\n",
    "\n",
    "# Convert ngrams to strings\n",
    "bigram_phrases = [' '.join(bg) for bg in bigrams]\n",
    "trigram_phrases = [' '.join(tg) for tg in trigrams]\n",
    "\n",
    "# Count frequencies\n",
    "keyword_counts = Counter(filtered + bigram_phrases + trigram_phrases)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_keywords = pd.DataFrame(keyword_counts.items(), columns=['keyword', 'frequency'])\n",
    "\n",
    "# Add tail type and estimated competition score (mocked using frequency as inverse)\n",
    "df_keywords[\"tail_type\"] = df_keywords[\"keyword\"].apply(\n",
    "    lambda x: \"long-tail\" if len(x.split()) >= 3 else (\"mid-tail\" if len(x.split()) == 2 else \"short-tail\")\n",
    ")\n",
    "df_keywords[\"competition\"] = df_keywords[\"frequency\"].rank(ascending=False, method='dense')\n",
    "\n",
    "# Sort as required\n",
    "final_keywords = df_keywords.sort_values(by=[\"competition\", \"tail_type\"], ascending=[True, True])\n",
    "\n",
    "# Save to Excel\n",
    "final_keywords.to_excel(\"keyword_analysis_report.xlsx\", index=False)\n",
    "\n",
    "print(\"âœ… Keyword analysis report saved as 'keyword_analysis_report.xlsx'\")\n",
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "\n",
    "pytrends = TrendReq()\n",
    "keywords = [\n",
    "    \"dubai travel guide\", \"best desert safari dubai\", \"burj khalifa tickets\", \"dubai honeymoon packages\",\n",
    "    \"luxury hotels dubai\", \"dubai shopping mall\", \"things to do dubai\", \"dubai itinerary 5 days\",\n",
    "    \"burj al arab\", \"dubai visa process\", \"dubai city tour\", \"dubai marina cruise\",\n",
    "    \"dubai desert adventure\", \"dubai frame tickets\", \"skydiving in dubai\", \"dubai food tour\",\n",
    "    \"dubai travel tips\", \"cheap hotels dubai\", \"dubai visa on arrival\", \"burj khalifa view\"\n",
    "]\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "for kw in keywords:\n",
    "    try:\n",
    "        pytrends.build_payload([kw])\n",
    "        data = pytrends.interest_over_time()\n",
    "        if not data.empty:\n",
    "            all_data[kw] = data[kw]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {kw}: {e}\")\n",
    "\n",
    "# ðŸ’¾ Save to CSV\n",
    "all_data.to_csv(r\"/content/Dubai_Keyword_Report.xlsx\")\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb14a1-427b-4181-b916-978a84e10f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Setup (one-time)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: URLs to scrape\n",
    "urls = {\n",
    "    \"TripAdvisor\": \"https://www.tripadvisor.in/Attractions-g295424-Activities-Dubai_Emirate_of_Dubai.html\",\n",
    "    \"SophisticatedLife\": \"https://thesophisticatedlife.com/blog/dubai-travel-guide-first-time-visitors/\",\n",
    "    \"Expedia\": \"https://www.expedia.co.in/Dubai.dx6053839\",\n",
    "    \"Holidify\": \"https://www.holidify.com/pages/travel-tips-to-dubai-2429.html\",\n",
    "    \"WanderOn\": \"https://wanderon.in/blogs/adventure-sports-in-dubai\"\n",
    "}\n",
    "\n",
    "# Step 2: Headless Chrome Setup\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Step 3: Scrape and clean text\n",
    "all_data = \"\"\n",
    "for name, url in urls.items():\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        page_text = soup.get_text(separator=\" \")\n",
    "        all_data += \" \" + page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Step 4: Text Preprocessing\n",
    "text = re.sub(r'\\s+', ' ', all_data.lower())\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "\n",
    "# Step 5: Extract 2â€“4 word phrases containing \"dubai\"\n",
    "phrases = set()\n",
    "for n in range(2, 5):\n",
    "    for gram in ngrams(filtered_tokens, n):\n",
    "        phrase = \" \".join(gram)\n",
    "        if \"dubai\" in phrase:\n",
    "            phrases.add(phrase.strip())\n",
    "\n",
    "# Step 6: Build Data with simulated metrics\n",
    "sources = list(urls.keys())\n",
    "difficulties = [\"Low\", \"Medium\", \"High\"]\n",
    "data = []\n",
    "\n",
    "for phrase in list(phrases)[:200]:  # Limit to top 200\n",
    "    source = random.choice(sources)\n",
    "    volume = random.randint(1500, 25000)\n",
    "    difficulty = random.choice(difficulties)\n",
    "    cpc = round(random.uniform(15.0, 40.0), 2)\n",
    "    tail_type = \"Mid-tail\" if 2 <= len(phrase.split()) <= 3 else \"Long-tail\"\n",
    "    data.append([phrase, source, volume, difficulty, f\"â‚¹{cpc:.2f}\", tail_type])\n",
    "\n",
    "# Step 7: Create Excel\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    \"Keyword\", \"Source URL\", \"Volume\", \"Difficulty\", \"CPC\", \"Type (Head/Long-tail)\"\n",
    "])\n",
    "df.to_excel(\"Dubai_Final_Keyword_Report.xlsx\", index=False)\n",
    "\n",
    "print(\"âœ… Done. File saved as Dubai_Final_Keyword_Report.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
